import argparse
import os
import re
import sys
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import Optional

import google.generativeai as genai

import transcribe_website
import summarize_analyses
import extract_seo

SCRIPT_DIR = Path(__file__).resolve().parent


def slugify(text: str) -> str:
    normalized = unicodedata.normalize("NFKC", text)
    lowered = normalized.lower()
    slug = re.sub(r"[^0-9a-zA-Zä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ¶ãƒ¼_]+", "_", lowered).strip("_")
    return slug or "default"


def _finalize_transcription(result: dict) -> Path:
    md_path = transcribe_website.save_markdown(result)
    transcribe_website.save_plain_text(result)
    transcribe_website.cleanup_segment_images(result)
    transcribe_website.update_latest_symlink(
        result["run_dir"],
        output_root=result.get("output_root"),
    )
    return md_path


def run_transcription(
    url: str,
    slice_height: int,
    overlap: int,
    keyword_slug: Optional[str] = None,
) -> Path:
    transcribe_website.ensure_ocr_ready()

    result = transcribe_website.transcribe_website(
        url=url,
        slice_height=slice_height,
        overlap=overlap,
        keyword_slug=keyword_slug,
    )

    return _finalize_transcription(result)


def run_local_transcription(
    html_path: Path,
    slice_height: int,
    overlap: int,
    keyword_slug: Optional[str] = None,
) -> Path:
    transcribe_website.ensure_ocr_ready()

    result = transcribe_website.transcribe_local_html(
        html_path=html_path,
        slice_height=slice_height,
        overlap=overlap,
        keyword_slug=keyword_slug,
    )

    return _finalize_transcription(result)


def generate_analysis_request(
    prompt_file: Path,
    transcript_path: Path,
    keyword: str,
    conversion_goal: str,
) -> Path:
    if not prompt_file.exists():
        raise FileNotFoundError(f"prompt file not found: {prompt_file}")
    if not transcript_path.exists():
        raise FileNotFoundError(f"transcript file not found: {transcript_path}")

    template = prompt_file.read_text(encoding="utf-8")
    transcript_text = transcript_path.read_text(encoding="utf-8").strip()

    filled = (
        template.replace("{{KEYWORD}}", keyword)
        .replace("{{CONVERSION_GOAL}}", conversion_goal)
        .replace("{{TRANSCRIPT}}", transcript_text)
    )

    output_path = transcript_path.parent / "analysis_request.md"
    output_path.write_text(filled, encoding="utf-8")
    return output_path


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "URL ã®æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œã—ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆã¨ Gemini ã«ã‚ˆã‚‹åˆ†æã‚’è¡Œã„ã¾ã™ã€‚"
        )
    )
    url_group = parser.add_mutually_exclusive_group(required=False)
    url_group.add_argument("--url", help="æ–‡å­—èµ·ã“ã—å¯¾è±¡ã® URL")
    url_group.add_argument(
        "--url-list",
        type=Path,
        help="URL ã®ä¸€è¦§ã‚’è¨˜è¼‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã€‚å„è¡Œã¾ãŸã¯ `URL:` è¡Œã«å«ã¾ã‚Œã‚‹ãƒªãƒ³ã‚¯ã‚’å¯¾è±¡ã«ã—ã¾ã™ã€‚",
    )
    parser.add_argument(
        "--local-html",
        type=Path,
        action="append",
        help="ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ãŸLP (HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª) ã‚’åˆ†æå¯¾è±¡ã¨ã—ã¦è¿½åŠ ã—ã¾ã™ã€‚è¤‡æ•°æŒ‡å®šã™ã‚‹ã«ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç¹°ã‚Šè¿”ã—ã¦ãã ã•ã„ã€‚",
    )
    parser.add_argument("--keyword", required=True, help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    parser.add_argument(
        "--conversion-goal",
        required=True,
        help="æƒ³å®šã—ã¦ã„ã‚‹ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆä¾‹: ç„¡æ–™ç›¸è«‡äºˆç´„ï¼‰",
    )
    parser.add_argument(
        "--slice-height",
        type=int,
        default=transcribe_website.SLICE_HEIGHT_DEFAULT,
        help="ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆåˆ†å‰²æ™‚ã®é«˜ã•(px)",
    )
    parser.add_argument(
        "--overlap",
        type=int,
        default=transcribe_website.SLICE_OVERLAP_DEFAULT,
        help="åˆ†å‰²ç”»åƒåŒå£«ã®é‡ãªã‚Š(px)",
    )
    parser.add_argument(
        "--prompt-file",
        type=Path,
        default=SCRIPT_DIR / "prompts/marketing_analysis_prompt.md",
        help="ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ",
    )
    parser.add_argument(
        "--gemini-model",
        default="models/gemini-2.5-flash",
        help="åˆ†æã«ä½¿ç”¨ã™ã‚‹ Gemini ãƒ¢ãƒ‡ãƒ« ID",
    )
    parser.add_argument(
        "--skip-gemini",
        action="store_true",
        help="Gemini ã«ã‚ˆã‚‹åˆ†æå®Ÿè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€analysis_request.md ã®ç”Ÿæˆã¾ã§ã§çµ‚äº†ã—ã¾ã™ã€‚",
    )
    parser.add_argument(
        "--summary-prompt-file",
        type=Path,
        default=SCRIPT_DIR / "prompts/consolidated_analysis_prompt.md",
        help="URLä¸€è¦§å‡¦ç†æ™‚ã«ä½¿ã†çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ",
    )
    parser.add_argument(
        "--summary-output",
        type=Path,
        default=None,
        help="çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›å…ˆã€‚æœªæŒ‡å®šã®å ´åˆã¯ runs-dir é…ä¸‹ã« timestamp ä»˜ãã§ä½œæˆ",
    )
    parser.add_argument(
        "--skip-summary",
        action="store_true",
        help="URLä¸€è¦§å‡¦ç†æ™‚ã«çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã›ã‚“ã€‚",
    )
    parser.add_argument(
        "--use-seo",
        action="store_true",
        help="æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®SEOä¸Šä½ã‚µã‚¤ãƒˆã‚’è‡ªå‹•æŠ½å‡ºã—ã€åˆ†æå¯¾è±¡ã«è¿½åŠ ã—ã¾ã™ã€‚",
    )
    parser.add_argument(
        "--seo-limit",
        type=int,
        default=extract_seo.RESULT_LIMIT,
        help=f"SEOæŠ½å‡ºã§å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•° (æ—¢å®š: {extract_seo.RESULT_LIMIT})",
    )

    args = parser.parse_args()

    if args.seo_limit <= 0:
        parser.error("--seo-limit ã«ã¯1ä»¥ä¸Šã®å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

    if not args.url and not args.url_list and not args.use_seo and not args.local_html:
        parser.error("--url / --url-list / --use-seo / --local-html ã®ã„ãšã‚Œã‹ã¯æŒ‡å®šãŒå¿…è¦ã§ã™ã€‚")

    if not args.prompt_file.is_absolute():
        args.prompt_file = (SCRIPT_DIR / args.prompt_file).resolve()
    if not args.summary_prompt_file.is_absolute():
        args.summary_prompt_file = (SCRIPT_DIR / args.summary_prompt_file).resolve()

    return args


def run_gemini_analysis(
    analysis_prompt_path: Path,
    model_name: str,
) -> Path:
    if not analysis_prompt_path.exists():
        raise FileNotFoundError(f"analysis prompt not found: {analysis_prompt_path}")

    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    genai.configure(api_key=api_key)

    prompt_text = analysis_prompt_path.read_text(encoding="utf-8")

    model = genai.GenerativeModel(model_name)
    response = model.generate_content(
        prompt_text,
        generation_config={"temperature": 0.2},
    )

    result_text = (
        transcribe_website.extract_text_from_genai_response(response).strip()
        if hasattr(transcribe_website, "extract_text_from_genai_response")
        else (response.text or "").strip()
    )

    if not result_text:
        raise RuntimeError("Gemini ã‹ã‚‰æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

    output_path = analysis_prompt_path.parent / "analysis_result_gemini.md"
    output_path.write_text(result_text, encoding="utf-8")
    return output_path


def extract_urls_from_file(url_file: Path) -> list[str]:
    import re

    text = url_file.read_text(encoding="utf-8")
    candidates = re.findall(r'https?://[^\s)>\]]+', text)
    urls: list[str] = []
    seen = set()
    for c in candidates:
        normalized = c.strip().rstrip('.,)')
        if normalized and normalized not in seen:
            seen.add(normalized)
            urls.append(normalized)
    return urls


def main() -> None:
    args = parse_args()

    keyword_slug = slugify(args.keyword)

    urls: list[str] = []
    url_metadata: dict[str, dict[str, str]] = {}
    local_html_entries: list[Path] = []

    if args.use_seo:
        print("ğŸ” SEOã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯çµæœã‚’æŠ½å‡ºä¸­...")
        try:
            seo_results = extract_seo.extract_organic_results(
                keyword=args.keyword,
                limit=args.seo_limit,
                keyword_slug=keyword_slug,
            )
            if seo_results:
                extract_seo.save_to_markdown(
                    seo_results,
                    args.keyword,
                    keyword_slug=keyword_slug,
                )
                for entry in seo_results:
                    normalized_url = (entry.get("url") or "").strip()
                    if not normalized_url:
                        continue
                    if normalized_url in url_metadata:
                        continue
                    urls.append(normalized_url)
                    url_metadata[normalized_url] = {
                        "source": "seo",
                        "title": entry.get("title", ""),
                        "snippet": entry.get("snippet", ""),
                    }
            else:
                print("âš ï¸ SEOæŠ½å‡ºã§æœ‰åŠ¹ãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        except Exception as error:
            print(f"âš ï¸ SEOæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {error}", file=sys.stderr)

    manual_urls: list[str] = []
    if args.url_list:
        if not args.url_list.exists():
            print(f"âŒ URLä¸€è¦§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.url_list}", file=sys.stderr)
            sys.exit(1)
        manual_urls = extract_urls_from_file(args.url_list)
        if not manual_urls:
            print(f"âŒ URLä¸€è¦§ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {args.url_list}", file=sys.stderr)
            sys.exit(1)
    elif args.url:
        manual_urls = [args.url.strip()]

    for manual_url in manual_urls:
        normalized = manual_url.strip()
        if not normalized:
            continue
        if normalized in url_metadata:
            continue
        urls.append(normalized)
        url_metadata[normalized] = {"source": "manual"}

    if args.local_html:
        for entry in args.local_html:
            try:
                resolved = transcribe_website.resolve_local_html_path(entry)
            except FileNotFoundError as error:
                print(f"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«LPã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {error}", file=sys.stderr)
                continue

            if resolved in local_html_entries:
                continue

            local_html_entries.append(resolved)

    if not urls and not local_html_entries:
        print("âŒ å‡¦ç†å¯¾è±¡ã¨ãªã‚‹URL/ãƒ­ãƒ¼ã‚«ãƒ«LPãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", file=sys.stderr)
        sys.exit(1)

    targets: list[dict[str, object]] = []

    print("å‡¦ç†å¯¾è±¡ä¸€è¦§:")
    for idx, target_url in enumerate(urls, start=1):
        meta = url_metadata.get(target_url, {})
        source_label = "SEOæŠ½å‡º" if meta.get("source") == "seo" else "æŒ‡å®šURL"
        print(f"  {idx}. {target_url} ({source_label})")
        if meta.get("title"):
            print(f"     ã‚¿ã‚¤ãƒˆãƒ«: {meta['title']}")
        targets.append({
            "type": "url",
            "value": target_url,
            "meta": meta,
            "label": source_label,
        })

    base_idx = len(targets)
    for offset, html_path in enumerate(local_html_entries, start=1):
        idx = base_idx + offset
        display_path = str(html_path)
        print(f"  {idx}. {display_path} (ãƒ­ãƒ¼ã‚«ãƒ«LP)")
        targets.append(
            {
                "type": "local_html",
                "value": html_path,
                "meta": {
                    "source": "local_html",
                    "html_path": str(html_path),
                },
                "label": "ãƒ­ãƒ¼ã‚«ãƒ«LP",
            }
        )

    overall_results = []

    total_targets = len(targets)

    for idx, target in enumerate(targets, start=1):
        print("=" * 80)

        target_type = target.get("type")
        meta = target.get("meta") or {}
        label = target.get("label") or ("ãƒ­ãƒ¼ã‚«ãƒ«LP" if target_type == "local_html" else "æŒ‡å®šURL")

        if target_type == "local_html":
            html_path = Path(target.get("value"))
            target_identifier = str(html_path)
            target_url = html_path.resolve().as_uri()
            print(f"[{idx}/{total_targets}] {target_identifier} (ãƒ­ãƒ¼ã‚«ãƒ«LP) ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        else:
            target_url = str(target.get("value"))
            target_identifier = target_url
            print(f"[{idx}/{total_targets}] {target_url} ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        try:
            if target_type == "local_html":
                transcript_path = run_local_transcription(
                    html_path=Path(target.get("value")),
                    slice_height=args.slice_height,
                    overlap=args.overlap,
                    keyword_slug=keyword_slug,
                )
            else:
                transcript_path = run_transcription(
                    url=target_url,
                    slice_height=args.slice_height,
                    overlap=args.overlap,
                    keyword_slug=keyword_slug,
                )
        except Exception as error:
            msg = f"æ–‡å­—èµ·ã“ã—å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({target_identifier}): {error}"
            print(f"âŒ {msg}", file=sys.stderr)
            overall_results.append(
                {
                    "url": target_url,
                    "html_path": target_identifier if target_type == "local_html" else None,
                    "source": meta.get("source") or ("local_html" if target_type == "local_html" else "manual"),
                    "title": meta.get("title"),
                    "snippet": meta.get("snippet"),
                    "label": label,
                    "success": False,
                    "error": msg,
                }
            )
            continue

        try:
            analysis_prompt_path = generate_analysis_request(
                prompt_file=args.prompt_file,
                transcript_path=transcript_path,
                keyword=args.keyword,
                conversion_goal=args.conversion_goal,
            )
        except Exception as error:
            msg = f"ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({target_identifier}): {error}"
            print(f"âŒ {msg}", file=sys.stderr)
            overall_results.append(
                {
                    "url": target_url,
                    "html_path": target_identifier if target_type == "local_html" else None,
                    "source": meta.get("source") or ("local_html" if target_type == "local_html" else "manual"),
                    "title": meta.get("title"),
                    "snippet": meta.get("snippet"),
                    "label": label,
                    "success": False,
                    "error": msg,
                }
            )
            continue

        analysis_result_path: Optional[Path] = None

        if not args.skip_gemini:
            try:
                analysis_result_path = run_gemini_analysis(
                    analysis_prompt_path=analysis_prompt_path,
                    model_name=args.gemini_model,
                )
            except Exception as error:
                print(f"âš ï¸ Gemini ã«ã‚ˆã‚‹åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ ({target_identifier}): {error}", file=sys.stderr)
            else:
                print("âœ… Gemini ã«ã‚ˆã‚‹ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        print("âœ… æ–‡å­—èµ·ã“ã—ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        print(f"  - æ–‡å­—èµ·ã“ã— Markdown: {transcript_path}")
        print(f"  - åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {analysis_prompt_path}")
        if analysis_result_path and analysis_result_path.exists():
            print(f"  - Gemini åˆ†æçµæœ: {analysis_result_path}")
        else:
            print("  - Gemini åˆ†æçµæœ: ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

        print("æ¬¡ã®æ‰‹é †:")
        if analysis_result_path and analysis_result_path.exists():
            print("  1. `analysis_result_gemini.md` ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦å…±æœ‰ã¾ãŸã¯æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚")
            print("  2. ã•ã‚‰ã« Claude ã§ã®å†åˆ†æãŒå¿…è¦ãªå ´åˆã¯ã€analysis_request.md ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚")
        else:
            print("  1. Claude Code ã‹ã‚‰ä¸Šè¨˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãã€ãã®å†…å®¹ã‚’ä¼šè©±ã«è²¼ã‚Šä»˜ã‘ã‚‹ã‹ã€")
            print("     MCP ã®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã§å†…å®¹ã‚’å…±æœ‰ã—ã¦ãã ã•ã„ã€‚")
            print("  2. å¿…è¦ã§ã‚ã‚Œã°åŒãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ†ã‚­ã‚¹ãƒˆç‰ˆã‚‚å‚ç…§ã§ãã¾ã™ã€‚")

        overall_results.append(
            {
                "url": target_url,
                "html_path": target_identifier if target_type == "local_html" else None,
                "source": meta.get("source") or ("local_html" if target_type == "local_html" else "manual"),
                "title": meta.get("title"),
                "snippet": meta.get("snippet"),
                "label": label,
                "success": True,
                "transcript": str(transcript_path),
                "analysis_prompt": str(analysis_prompt_path),
                "analysis_result": str(analysis_result_path) if analysis_result_path else None,
            }
        )

    print("=" * 80)
    print("å‡¦ç†ã‚µãƒãƒª:")
    for result in overall_results:
        source_label = result.get("label") or (
            "SEOæŠ½å‡º"
            if result.get("source") == "seo"
            else ("ãƒ­ãƒ¼ã‚«ãƒ«LP" if result.get("source") == "local_html" else "æŒ‡å®šURL")
        )
        if result.get("success"):
            print(f"âœ… {result['url']} ({source_label})")
            if result.get("title"):
                print(f"   - title: {result['title']}")
            if result.get("html_path"):
                print(f"   - html_path: {result['html_path']}")
            print(f"   - transcript: {result['transcript']}")
            print(f"   - analysis_prompt: {result['analysis_prompt']}")
            if result.get("analysis_result"):
                print(f"   - analysis_result: {result['analysis_result']}")
            else:
                print("   - analysis_result: ãªã—")
        else:
            print(f"âŒ {result['url']} ({source_label})")
            if result.get("title"):
                print(f"   - title: {result['title']}")
            if result.get("html_path"):
                print(f"   - html_path: {result['html_path']}")
            print(f"   - error: {result['error']}")

    if not any(r.get("success") for r in overall_results):
        sys.exit(1)

    analysis_ready = [
        r for r in overall_results if r.get("success") and r.get("analysis_result")
    ]

    if args.url_list and not args.skip_summary and analysis_ready:
        try:
            first_result_path = Path(analysis_ready[0]["analysis_result"])
            runs_dir = first_result_path.parent.parent
            latest_count = len(analysis_ready)
            entries = summarize_analyses.collect_analysis_entries(runs_dir, latest_count)
            prompt_text = summarize_analyses.build_prompt(args.summary_prompt_file, entries)
            summary_text = summarize_analyses.run_gemini(prompt_text, args.gemini_model)
        except Exception as error:
            print(f"âš ï¸ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {error}", file=sys.stderr)
        else:
            if args.summary_output:
                summary_path = args.summary_output
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                summary_path = runs_dir / f"consolidated_analysis_{timestamp}.md"
            summary_path.parent.mkdir(parents=True, exist_ok=True)
            summary_path.write_text(summary_text, encoding="utf-8")
            print("âœ… çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
            print(f"   - ä¿å­˜å…ˆ: {summary_path}")


if __name__ == "__main__":
    main()
