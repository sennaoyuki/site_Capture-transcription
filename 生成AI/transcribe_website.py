"""
Webã‚µã‚¤ãƒˆã®æ–‡å­—èµ·ã“ã—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚€ï¼‰
Playwrightã¨OCRã‚’ä½¿ç”¨ã—ã¦ã€Webãƒšãƒ¼ã‚¸ã®ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã™
"""
from playwright.sync_api import sync_playwright
import time
import os
import base64
from pathlib import Path
from datetime import datetime
import json
import re

try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("âš ï¸ OCRãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã«ã¯ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š")
    print("  pip install pillow pytesseract")
    print("  Tesseractã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆMacï¼‰: brew install tesseract tesseract-lang")


def extract_all_text_from_website(url):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    - ãƒšãƒ¼ã‚¸å†…ã®å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆ
    - ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆOCRï¼‰
    """
    
    result = {
        'url': url,
        'timestamp': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S'),
        'visible_text': '',
        'images_text': [],
        'meta_info': {},
        'all_text_combined': ''
    }
    
    with sync_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•
        browser = p.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
            ]
        )
        
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
            locale='ja-JP',
            timezone_id='Asia/Tokyo'
        )
        
        page = context.new_page()
        
        try:
            print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ä¸­: {url}")
            page.goto(url, wait_until="networkidle", timeout=60000)
            
            # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            time.sleep(3)
            
            # ãƒšãƒ¼ã‚¸ã‚’å°‘ã—ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦é…å»¶èª­ã¿è¾¼ã¿è¦ç´ ã‚’è¡¨ç¤º
            page.evaluate("""
                () => {
                    window.scrollTo(0, document.body.scrollHeight / 3);
                }
            """)
            time.sleep(2)
            
            page.evaluate("""
                () => {
                    window.scrollTo(0, document.body.scrollHeight * 2 / 3);
                }
            """)
            time.sleep(2)
            
            page.evaluate("""
                () => {
                    window.scrollTo(0, document.body.scrollHeight);
                }
            """)
            time.sleep(2)
            
            # æœ€ä¸Šéƒ¨ã«æˆ»ã‚‹
            page.evaluate("window.scrollTo(0, 0)")
            time.sleep(1)
            
            print("âœ… ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            # ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—
            print("\nğŸ“‹ ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—ä¸­...")
            result['meta_info']['title'] = page.title()
            
            # ãƒ¡ã‚¿ã‚¿ã‚°ã‚’å–å¾—
            meta_description = page.locator('meta[name="description"]').get_attribute('content') if page.locator('meta[name="description"]').count() > 0 else ""
            meta_keywords = page.locator('meta[name="keywords"]').get_attribute('content') if page.locator('meta[name="keywords"]').count() > 0 else ""
            
            result['meta_info']['description'] = meta_description or ""
            result['meta_info']['keywords'] = meta_keywords or ""
            
            # å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            print("\nğŸ“ ãƒšãƒ¼ã‚¸å†…ã®å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­...")
            
            # bodyå…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆJavaScriptã§æ•´å½¢ã—ã¦å–å¾—ï¼‰
            visible_text = page.evaluate("""
                () => {
                    // ä¸è¦ãªè¦ç´ ã‚’é™¤å¤–
                    const excludeSelectors = ['script', 'style', 'noscript', 'iframe'];
                    excludeSelectors.forEach(selector => {
                        document.querySelectorAll(selector).forEach(el => el.remove());
                    });
                    
                    // bodyã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    const text = document.body.innerText;
                    
                    // è¤‡æ•°ã®é€£ç¶šã™ã‚‹ç©ºç™½ãƒ»æ”¹è¡Œã‚’æ•´ç†
                    return text.replace(/\\n\\s*\\n/g, '\\n\\n').trim();
                }
            """)
            
            result['visible_text'] = visible_text
            print(f"âœ… å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†ï¼ˆ{len(visible_text)}æ–‡å­—ï¼‰")
            
            # ç”»åƒã‚’æŠ½å‡ºã—ã¦OCR
            if OCR_AVAILABLE:
                print("\nğŸ–¼ï¸  ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­ï¼ˆOCRï¼‰...")
                
                # ã™ã¹ã¦ã®ç”»åƒè¦ç´ ã‚’å–å¾—
                images = page.locator('img').all()
                print(f"   è¦‹ã¤ã‹ã£ãŸç”»åƒæ•°: {len(images)}")
                
                # ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
                work_dir = Path("/Users/hattaryoga/Library/CloudStorage/GoogleDrive-naoyuki.uebayashi@senjinholdings.com/ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–/1_ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘ŠæŠœãå‡ºã—/ç”ŸæˆAI/temp_images")
                work_dir.mkdir(exist_ok=True)
                
                for idx, img in enumerate(images, 1):
                    try:
                        # ç”»åƒãŒå¯è¦–ã‹ãƒã‚§ãƒƒã‚¯
                        if not img.is_visible():
                            continue
                        
                        # ç”»åƒã®ã‚µã‚¤ã‚ºã‚’å–å¾—ï¼ˆå°ã•ã™ãã‚‹ç”»åƒã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                        box = img.bounding_box()
                        if not box or box['width'] < 50 or box['height'] < 50:
                            continue
                        
                        # ç”»åƒã®altå±æ€§ã¨srcå±æ€§ã‚’å–å¾—
                        alt_text = img.get_attribute('alt') or ""
                        src = img.get_attribute('src') or ""
                        
                        print(f"   å‡¦ç†ä¸­: ç”»åƒ {idx}/{len(images)}", end="\r")
                        
                        # ç”»åƒã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
                        img_path = work_dir / f"image_{idx}.png"
                        img.screenshot(path=str(img_path))
                        
                        # OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                        try:
                            image = Image.open(img_path)
                            # æ—¥æœ¬èªã¨è‹±èªã§OCR
                            ocr_text = pytesseract.image_to_string(image, lang='jpn+eng')
                            ocr_text = ocr_text.strip()
                            
                            if ocr_text:
                                result['images_text'].append({
                                    'index': idx,
                                    'alt': alt_text,
                                    'src': src[:100] if src else "",  # URLçœç•¥è¡¨ç¤º
                                    'ocr_text': ocr_text
                                })
                        except Exception as e:
                            print(f"\n   âš ï¸ ç”»åƒ {idx} ã®OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        
                        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
                        img_path.unlink(missing_ok=True)
                        
                    except Exception as e:
                        print(f"\n   âš ï¸ ç”»åƒ {idx} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                print(f"\nâœ… OCRå®Œäº†: {len(result['images_text'])}å€‹ã®ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º")
                
                # ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
                try:
                    work_dir.rmdir()
                except:
                    pass
            else:
                print("\nâš ï¸ OCRãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            
            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜
            screenshot_path = "/Users/hattaryoga/Library/CloudStorage/GoogleDrive-naoyuki.uebayashi@senjinholdings.com/ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–/1_ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘ŠæŠœãå‡ºã—/ç”ŸæˆAI/website_screenshot.png"
            page.screenshot(path=screenshot_path, full_page=True)
            print(f"\nğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜: website_screenshot.png")
            
            # HTMLã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜
            html_path = "/Users/hattaryoga/Library/CloudStorage/GoogleDrive-naoyuki.uebayashi@senjinholdings.com/ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–/1_ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘ŠæŠœãå‡ºã—/ç”ŸæˆAI/website_source.html"
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(page.content())
            print(f"ğŸ’¾ HTMLã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜: website_source.html")
            
            # ç¢ºèªã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            print("\nğŸ” çµæœã‚’ç¢ºèªã™ã‚‹ãŸã‚ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‚’5ç§’é–“é–‹ã„ãŸã¾ã¾ã«ã—ã¾ã™...")
            time.sleep(5)
            
        except Exception as e:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            browser.close()
    
    # ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    all_text_parts = [result['visible_text']]
    
    if result['images_text']:
        all_text_parts.append("\n\n=== ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆ ===\n")
        for img_data in result['images_text']:
            if img_data['alt']:
                all_text_parts.append(f"\n[ç”»åƒ {img_data['index']} - alt: {img_data['alt']}]")
            else:
                all_text_parts.append(f"\n[ç”»åƒ {img_data['index']}]")
            all_text_parts.append(f"\n{img_data['ocr_text']}\n")
    
    result['all_text_combined'] = "\n".join(all_text_parts)
    
    return result


def save_to_markdown(result):
    """
    æŠ½å‡ºçµæœã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    md_path = "/Users/hattaryoga/Library/CloudStorage/GoogleDrive-naoyuki.uebayashi@senjinholdings.com/ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–/1_ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘ŠæŠœãå‡ºã—/ç”ŸæˆAI/website_transcription.md"
    
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(f"# Webã‚µã‚¤ãƒˆæ–‡å­—èµ·ã“ã—çµæœ\n\n")
        f.write(f"**URL:** {result['url']}\n\n")
        f.write(f"**æŠ½å‡ºæ—¥æ™‚:** {result['timestamp']}\n\n")
        
        # ãƒ¡ã‚¿æƒ…å ±
        if result['meta_info']:
            f.write(f"## ãƒ¡ã‚¿æƒ…å ±\n\n")
            f.write(f"**ã‚¿ã‚¤ãƒˆãƒ«:** {result['meta_info'].get('title', 'N/A')}\n\n")
            if result['meta_info'].get('description'):
                f.write(f"**èª¬æ˜:** {result['meta_info']['description']}\n\n")
            if result['meta_info'].get('keywords'):
                f.write(f"**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:** {result['meta_info']['keywords']}\n\n")
        
        f.write("---\n\n")
        
        # å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆ
        f.write(f"## ãƒšãƒ¼ã‚¸å†…ãƒ†ã‚­ã‚¹ãƒˆ\n\n")
        f.write(f"```\n{result['visible_text']}\n```\n\n")
        
        # ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆ
        if result['images_text']:
            f.write(f"## ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆï¼ˆOCRæŠ½å‡ºï¼‰\n\n")
            f.write(f"æŠ½å‡ºã•ã‚ŒãŸç”»åƒæ•°: {len(result['images_text'])}\n\n")
            
            for img_data in result['images_text']:
                f.write(f"### ç”»åƒ {img_data['index']}\n\n")
                if img_data['alt']:
                    f.write(f"**Altå±æ€§:** {img_data['alt']}\n\n")
                if img_data['src']:
                    f.write(f"**ç”»åƒURL:** `{img_data['src']}`\n\n")
                f.write(f"**æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ:**\n\n")
                f.write(f"```\n{img_data['ocr_text']}\n```\n\n")
                f.write("---\n\n")
        
        # çµ±åˆãƒ†ã‚­ã‚¹ãƒˆ
        f.write(f"## ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆçµ±åˆç‰ˆï¼‰\n\n")
        f.write(f"```\n{result['all_text_combined']}\n```\n\n")
    
    print(f"\nâœ… Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: website_transcription.md")
    return md_path


def save_to_text(result):
    """
    æŠ½å‡ºçµæœã‚’ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    txt_path = "/Users/hattaryoga/Library/CloudStorage/GoogleDrive-naoyuki.uebayashi@senjinholdings.com/ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–/1_ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘ŠæŠœãå‡ºã—/ç”ŸæˆAI/website_transcription.txt"
    
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(f"Webã‚µã‚¤ãƒˆæ–‡å­—èµ·ã“ã—çµæœ\n")
        f.write(f"{'=' * 60}\n\n")
        f.write(f"URL: {result['url']}\n")
        f.write(f"æŠ½å‡ºæ—¥æ™‚: {result['timestamp']}\n\n")
        
        if result['meta_info']:
            f.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['meta_info'].get('title', 'N/A')}\n\n")
        
        f.write(f"{'=' * 60}\n\n")
        f.write(result['all_text_combined'])
    
    print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: website_transcription.txt")
    return txt_path


if __name__ == "__main__":
    print("=" * 70)
    print("ğŸŒ Webã‚µã‚¤ãƒˆæ–‡å­—èµ·ã“ã—ãƒ„ãƒ¼ãƒ«ï¼ˆç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œï¼‰")
    print("=" * 70)
    print()
    
    # å¯¾è±¡URL
    url = "https://l.shift-ai.co.jp/lp/lpcam01a/?free4=SY_GSN_lpCAM01A_11111"
    
    if not OCR_AVAILABLE:
        print("âš ï¸ OCRæ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯:")
        print("   1. pip install pillow pytesseract")
        print("   2. brew install tesseract tesseract-lang (Macã®å ´åˆ)")
        print()
        response = input("OCRãªã—ã§ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ")
        if response.lower() != 'y':
            print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ")
            exit()
    
    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’å®Ÿè¡Œ
    result = extract_all_text_from_website(url)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    md_file = save_to_markdown(result)
    txt_file = save_to_text(result)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ å‡¦ç†å®Œäº†ï¼")
    print("=" * 70)
    print(f"\nğŸ“Š æŠ½å‡ºçµæœ:")
    print(f"   - å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆ: {len(result['visible_text'])} æ–‡å­—")
    print(f"   - ç”»åƒã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º: {len(result['images_text'])} å€‹")
    print(f"   - åˆè¨ˆãƒ†ã‚­ã‚¹ãƒˆ: {len(result['all_text_combined'])} æ–‡å­—")
    print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   - {md_file}")
    print(f"   - {txt_file}")
    print()


